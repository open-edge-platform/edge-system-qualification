{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"about/","title":"About","text":"<p>About ESQ and project information.</p> <p>Documentation in progress.</p>"},{"location":"about/changelog/","title":"Changelog","text":"<p>Intel\u00ae ESQ changelog and version history.</p>"},{"location":"about/changelog/#version-202520-december-2025","title":"Version 2025.2.0 - December 2025","text":""},{"location":"about/changelog/#features","title":"Features","text":"<ul> <li>New qualification profile with Gen AI and Vision AI test cases</li> <li>New manufacturing-focused detection &amp; classification pipelines</li> <li>New retail-focused detection pipeline</li> <li>New system memory test suites</li> <li>CSV attachment visualization and improved vertical benchmarking sections</li> <li>Support for versioned docs</li> <li>Automatically execute prerequisite profiles</li> <li>Extractor utility for Allure test results</li> <li>Update CLI run command with opt-out prompt for vertical profiles</li> <li>Enable multiple video format conversion including h265</li> <li>Enable YOLO model export with dynamic shape and kagglehub resnet support</li> <li>Enable DL Streamer pytest with configurable sync element for benchmarking analysis</li> </ul>"},{"location":"about/changelog/#version-202510-october-2025","title":"Version 2025.1.0 - October 2025","text":""},{"location":"about/changelog/#new-features","title":"New Features","text":"<ul> <li>Framework Revamp: Rebuilt Intel\u00ae ESQ with new, more robust architecture</li> <li>GenAI Testing: New generative AI test suite for AI workload evaluation</li> <li>VisionAI Testing: Comprehensive computer vision testing pipeline</li> <li>System Detection: Automatic software configuration detection and reporting</li> <li>Enhanced Logging: Detailed execution logs for each test module</li> <li>Timeout Management: Automatic test termination when time limits are exceeded</li> </ul>"},{"location":"about/changelog/#notes","title":"Notes","text":"<p>This major release establishes the foundation for future Intel\u00ae ESQ development with breaking changes from previous versions.</p>"},{"location":"about/license/","title":"License","text":"<p>ESQ is licensed under the Apache License 2.0.</p> <p>Documentation in progress.</p>"},{"location":"about/support/","title":"Support","text":"<p>ESQ support and help resources.</p> <p>Documentation in progress.</p>"},{"location":"assets/","title":"MkDocs Assets","text":"<p>This directory contains assets for the documentation:</p> <ul> <li><code>images/</code> - Images, logos, and graphics</li> <li><code>stylesheets/</code> - Custom CSS files</li> <li><code>javascripts/</code> - Custom JavaScript files</li> </ul>"},{"location":"assets/#placeholder-files","title":"Placeholder Files","text":"<p>The following files would be added in a production setup:</p> <ul> <li><code>images/intel-logo.svg</code> - Intel logo for the site header</li> <li><code>images/favicon.png</code> - Site favicon</li> <li><code>stylesheets/extra.css</code> - Custom styling</li> <li><code>javascripts/extra.js</code> - Custom JavaScript</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This section will guide you through the complete setup of Intel\u00ae ESQ and introduce the available features you'll use to evaluate your edge system.</p> <ol> <li>Quick Start \u2013 Install dependencies and run all tests</li> </ol>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<p>If you encounter issues during setup:</p> <ol> <li>Refer to the Optimization guide</li> <li>Check the Troubleshooting guide</li> <li>Visit GitHub* Issues for community support</li> </ol>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Install all required dependencies to run all tests and evaluate your edge system.</p> <p>Info</p> <p>Refer to Intel\u00ae ESQ for Intel\u00ae AI Edge Systems for official main qualification page.</p>"},{"location":"getting-started/quick-start/#requirements","title":"Requirements","text":"<p>Before starting, ensure your system meets the following requirements:</p>"},{"location":"getting-started/quick-start/#1-hardware","title":"1. Hardware","text":"<p>Intel\u00ae ESQ supports a wide range of Intel\u00ae edge systems optimized for various performance and use case requirements.</p> Category Processors Specifications Storage Discrete GPU Options Scalable Performance Graphics Media Xeon-Based: Intel\u00ae Xeon\u00ae 6 Processors,5<sup>th</sup> Gen Intel\u00ae Xeon\u00ae Scalable Processors,Intel\u00ae Xeon\u00ae W ProcessorsCore-Based: Intel\u00ae Core Ultra Series 2, Intel\u00ae Core\u2122 Series 2 Xeon-Based: SKU: Dual and Single SocketMinimum System Memory: 128 GB DDR5 (Dual Channel) Core-Based: Minimum System Memory: 64GB DDR5 (Dual Channel) Recommended: 1TB Intel\u00ae Arc\u2122 B-Series Graphics,Intel\u00ae Arc\u2122 Pro B-Series Graphics Scalable Performance Intel\u00ae Xeon\u00ae 6 Processors, 5<sup>th</sup> Gen Intel\u00ae Xeon\u00ae Scalable Processors, Intel\u00ae Xeon\u00ae W Processors SKU: Dual and Single SocketMinimum System Memory: 128 GB DDR5 (Dual Channnel)  Recommended: 1TB N/A Efficiency Optimized Intel\u00ae Core\u2122 Ultra processor Series 2 Graphics: Integrated GPU with 7 Xe-Cores or more  SKU: Single Socket Minimum System Memory: 32 GB DDR5 (Dual Channel) Recommended: 512 GB N/A Mainstream Intel\u00ae Core\u2122 Series 2 Minimum System Memory: 32 GB DDR5 (Dual Channel) Recommended: 512 GB N/A Entry Intel\u00ae Processor for Desktop,Intel\u00ae Processor X-series, Intel\u00ae Processor N-series Minimum System Memory: 32 GB DDR5 (Dual Channel) Recommended: 512 GB N/A"},{"location":"getting-started/quick-start/#2-operating-system","title":"2. Operating System","text":"<p>Install a supported operating system before proceeding.</p> OS Version Notes Ubuntu* 24.04 Desktop LTS or newer Recommended Linux* distribution Windows* Coming soon Coming soon"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":""},{"location":"getting-started/quick-start/#1-system-drivers","title":"1. System Drivers","text":"<p>Configure system drivers:</p> <pre><code>sudo bash -c \"$(wget -qLO - https://raw.githubusercontent.com/open-edge-platform/edge-developer-kit-reference-scripts/refs/heads/main/main_installer.sh)\"\n</code></pre> <p>Additional Reference</p> <p>For detailed information about system drivers, see the Edge Developer Kit Reference Scripts documentation.</p>"},{"location":"getting-started/quick-start/#2-system-dependencies","title":"2. System Dependencies","text":"<p>Install essential system packages:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y curl git\n</code></pre>"},{"location":"getting-started/quick-start/#3-docker-engine","title":"3. Docker Engine","text":"<p>Install Docker* Engine:</p> <pre><code># Add Docker's official GPG key\nsudo apt-get update\nsudo apt-get install ca-certificates curl\nsudo install -m 0755 -d /etc/apt/keyrings\nsudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\nsudo chmod a+r /etc/apt/keyrings/docker.asc\n\n# Add the repository to Apt sources\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n  $(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\") stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\nsudo apt-get update\n\n# Install Docker packages\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre> <p>Add your user to the docker group:</p> <p>Warning</p> <p>Adding your user to the <code>docker</code> group grants root-level access. This should only be done on development or test systems.</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>To activate your new group membership immediately in your current terminal, run:</p> <pre><code>newgrp docker\n</code></pre> <p>This command starts a new shell session with updated group permissions, allowing you to use Docker* without logging out. You can now verify Docker* installation:</p> <pre><code>docker ps\n</code></pre> <p>Note</p> <p>If you see a list of containers (even if empty), your user is correctly added to the <code>docker</code> group. If you get a permission error, ensure you have run <code>newgrp docker</code> in your terminal. For persistent access across all sessions, log out and log back in, or reboot your system.</p> <p>Additional Reference</p> <p>For detailed Docker* installation instructions, see the official Docker* Engine installation documentation.</p>"},{"location":"getting-started/quick-start/#4-python-package-manager","title":"4. Python Package Manager","text":"<p>Install uv to accelerate Python* package management:</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh &amp;&amp; source $HOME/.local/bin/env\n</code></pre> <p>Additional Reference</p> <p>For detailed <code>uv</code> installation instructions, see the official uv Installation documentation.</p>"},{"location":"getting-started/quick-start/#5-platform-power-monitoring","title":"5. Platform Power Monitoring","text":"<p>Configure non-root access to RAPL (Running Average Power Limit) powercap files for platform power monitoring:</p> <p>About RAPL Power Monitoring</p> <p>RAPL provides energy consumption data for Intel\u00ae processors. This step enables Intel\u00ae ESQ to collect power configuration information without requiring root privileges.</p> <p>Run the automated setup script to configure group-based permissions:</p> <pre><code>sudo bash -c \"$(wget -qLO - https://raw.githubusercontent.com/open-edge-platform/edge-system-qualification/refs/heads/main/scripts/setup-powercap-permissions.sh)\"\n</code></pre> <p>This setup script:</p> <ul> <li>Auto-detects all powercap energy files on your system</li> <li>Creates a <code>powercap</code> group for secure, user-specific access</li> <li>Adds your user to the <code>powercap</code> group</li> <li>Configures persistent permissions via <code>/etc/sysfs.d/powercap.conf</code> (applied automatically on boot)</li> <li>Applies permissions immediately for the current session</li> </ul> <p>Security &amp; Persistence</p> <p>This configuration uses group-based permissions (mode 0440, owner root:powercap) to grant read-only access to RAPL powercap files exclusively to users in the <code>powercap</code> group. Permissions are automatically reapplied on boot via the sysfs configuration.</p> <p>Verification</p> <p>Verify access with: <pre><code>cat /sys/class/powercap/intel-rapl/intel-rapl:0/energy_uj\n</code></pre> If this command prints a number without requiring sudo, the setup was successful.</p>"},{"location":"getting-started/quick-start/#6-intel-esq","title":"6. Intel\u00ae ESQ","text":"<p>Install Intel\u00ae ESQ from GitHub*:</p> <pre><code>uv tool install --force --refresh git+https://github.com/open-edge-platform/edge-system-qualification.git@main\n</code></pre> <p>Verify that ESQ is working correctly:</p> <pre><code>esq --version\n</code></pre>"},{"location":"getting-started/quick-start/#quick-start_1","title":"Quick Start","text":"<p>Run all tests and review the generated test report:</p> <p>Newer Version</p> <p>Before running a new version of ESQ, run the following command to clean up any previously created <code>esq_data</code> folder:</p> <pre><code>esq clean --all\n</code></pre> <p>This ensures that leftover data from previous ESQ versions does not interfere with the new installation. If you have uninstalled ESQ but the <code>esq_data</code> folder still exists, remove it using the above command before running any new ESQ commands. Otherwise, ESQ may not work as expected.</p> <p>Alternative Download Source</p> <p>Intel\u00ae ESQ uses HuggingFace* as the default download source. If you prefer to use an alternative source such as ModelScope*, you can configure it with:</p> <pre><code>export PREFER_MODELSCOPE=1\n</code></pre>"},{"location":"getting-started/quick-start/#1-run-intel-esq","title":"1. Run Intel\u00ae ESQ","text":"<p>Run qualification and vertical (recommended) test profiles to generate a test report:</p> <pre><code>esq run\n</code></pre> <p>By default, this command will:</p> <ol> <li>Run qualification profiles (always included)</li> <li>Run vertical profiles (unless you choose to skip them at the prompt)</li> <li>Collect metrics and generate a test report</li> </ol> <p>Verbose Output</p> <p>Use the <code>--verbose</code> option to see detailed information while running tests:</p> <pre><code>esq --verbose run\n</code></pre> <p>This provides real-time feedback on test progress, system information collection, and detailed execution logs.</p> <p>Run All Profile Types</p> <p>Use the <code>--all</code> option to run all available profile types (qualification, vertical, and suite):</p> <pre><code>esq --verbose run --all\n</code></pre> <p>Driver Requirements</p> <p>Intel\u00ae GPU and NPU tests require specific drivers. Ensure you have the latest Intel\u00ae drivers installed for your hardware configuration.</p> <p>Virtualization</p> <p>Running in virtual machines may impact performance and hardware acceleration capabilities. Bare metal installation is recommended for accurate testing results.</p> <p>Ready to explore all test suites? Continue to the Test Suites \u2192</p>"},{"location":"getting-started/suites/","title":"Test Suites","text":"<p>Intel\u00ae ESQ provides a comprehensive collection of test suites to assess and qualify your edge system capabilities. Choose from qualification tests with pass/fail criteria, data collection suites for analysis, or industry-specific vertical tests.</p>"},{"location":"getting-started/suites/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Available Test Suites</li> <li>Test Suite Types</li> <li>Qualifications<ul> <li>AI Edge System Qualification</li> </ul> </li> <li>Vertical<ul> <li>Manufacturing</li> <li>Metro</li> <li>Retail<ul> <li>Automated Self Checkout</li> <li>Loss Prevention</li> </ul> </li> </ul> </li> <li>Horizontal<ul> <li>Generative AI</li> <li>Vision AI<ul> <li>DL Streamer Analysis - Multi-Stream Pipelines With Multiple AI Stages</li> <li>DL Streamer Analysis - Verified Reference Blueprints</li> <li>OpenVINO</li> </ul> </li> <li>System GPU - OpenVINO</li> <li>System Memory - STREAM</li> <li>Media Performance</li> </ul> </li> </ul>"},{"location":"getting-started/suites/#available-test-suites","title":"Available Test Suites","text":"<p>Quick reference of all available test suites and their profile names.</p> Profile Name Category Description Run Command <code>profile.qualification.ai-edge-system</code> Qualification AI Edge System qualification <code>esq run --profile profile.qualification.ai-edge-system</code> <code>profile.vertical.manufacturing</code> Vertical Manufacturing <code>esq run --profile profile.vertical.manufacturing</code> <code>profile.vertical.metro</code> Vertical Metro <code>esq run --profile profile.vertical.metro</code> <code>profile.vertical.retail-asc</code> Vertical Retail Automated Self-Checkout <code>esq run --profile profile.vertical.retail-asc</code> <code>profile.vertical.retail-lp</code> Vertical Retail Loss Prevention <code>esq run --profile profile.vertical.retail-lp</code> <code>profile.suite.ai.gen</code> Horizontal Gen AI profile <code>esq run --profile profile.suite.ai.gen</code> <code>profile.suite.ai.vision-light</code> Horizontal DL Streamer Analysis - Multi-Stream Pipelines With Multiple AI Stages <code>esq run --profile profile.suite.ai.vision-light</code> <code>profile.suite.ai.vision-ov</code> Horizontal OpenVINO Benchmark - Measures raw inference performance using OpenVINO Runtime API <code>esq run --profile profile.suite.ai.vision-ov</code> <code>profile.suite.ai.vision-vrb</code> Horizontal Vision AI profile - Verified Reference Blueprints <code>esq run --profile profile.suite.ai.vision-vrb</code> <code>profile.suite.system.gpu-ov</code> Horizontal System GPU Performance using OpenVINO benchmark <code>esq run --profile profile.suite.system.gpu-ov</code> <code>profile.suite.system.memory-stream</code> Horizontal System Memory Performance using STREAM benchmark <code>esq run --profile profile.suite.system.memory-stream</code> <code>profile.suite.media.performance-pipelines</code> Horizontal Media Performance <code>esq run --profile profile.suite.media.performance-pipelines</code> <p>List all available profiles: <pre><code>esq list\n</code></pre></p>"},{"location":"getting-started/suites/#test-suite-types","title":"Test Suite Types","text":"Test Suite Purpose Benefit Qualifications Measuring system performance to qualify against AI Edge Systems Qualifications Metrics Gain Catalog inclusion and other marketing benefits from Intel. Vertical System benchmarking vertical specific proxy workloads like retail self checkout, smart NVR and manufacturing defect detection Gain understanding and communicate on system's potential to be used in a variety of verticals and use-cases Horizontal General system benchmarking (includes OpenVINO\u2122, Audio, Memory Performance) Gain understanding on system's resource utilization and performance like System memory and GPU during select AI workload"},{"location":"getting-started/suites/#qualifications","title":"Qualifications","text":""},{"location":"getting-started/suites/#ai-edge-system-qualification","title":"AI Edge System Qualification","text":"<p>Profile: <code>profile.qualification.ai-edge-system</code></p> <p>Test Cases:</p> <p>Generative AI test on text generation</p> Tier Test ID Test Case Qualification Criteria Entry AES-GEN-001 Gen AI LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-1.5B INT4 &gt;= 10.0 tokens/sec Mainstream AES-GEN-001 Gen AI LLM Serving Benchmark - Phi-4-mini-reasoning 3.8B INT4 &gt;= 10.0 tokens/sec Efficiency Optimized AES-GEN-001 Gen AI LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-7B INT4 &gt;= 10.0 tokens/sec Scalable Performance AES-GEN-001 Gen AI LLM Serving Benchmark - Qwen3-32B INT4 &gt;= 10.0 tokens/sec Scalable Performance Graphics Media AES-GEN-001 Gen AI LLM Serving Benchmark - Phi-4-mini-reasoning 3.8B INT4Gen AI LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-14B INT4Gen AI LLM Serving Benchmark - Qwen3-32B INT4 &gt;= 10.0 tokens/sec <p>Vision AI test using Intel\u00ae DLStreamer</p> Tier Test ID Test Case Qualification Criteria Entry AES-VSN-001 Vision AI Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 &gt;=  4.0 streams Mainstream AES-VSN-001 Vision AI Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 &gt;=  8.0 streams Efficiency Optimized AES-VSN-001 Vision AI Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 &gt;=  25.0 streams Scalable Performance AES-VSN-001 Vision AI Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 &gt;=  10.0 streams Scalable Performance Graphics Media AES-VSN-001 Vision AI Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 &gt;=  40.0 streams <p>Run this profile: <pre><code>esq run --profile profile.qualification.ai-edge-system\n</code></pre></p>"},{"location":"getting-started/suites/#vertical","title":"Vertical","text":""},{"location":"getting-started/suites/#manufacturing","title":"Manufacturing","text":"<p>Profile: <code>profile.vertical.manufacturing</code></p> <p>Test Case:</p> Test ID Test Case MFG-PDD-001 Pallet Defect Detection - multi-stream 480p30 H.264 gvadetect YOLOX-TINY FP32 (CPU) MFG-PDD-002 Pallet Defect Detection - multi-stream 480p30 H.264 gvadetect YOLOX-TINY FP32 (iGPU) MFG-PDD-003 Pallet Defect Detection - multi-stream 480p30 H.264 gvadetect YOLOX-TINY FP32 (dGPU) MFG-WPC-001 Weld Porosity Classification - multi-stream 1024p30 H.264 gvaclassify EfficientNet-B0 FP16 (CPU) MFG-WPC-002 Weld Porosity Classification - multi-stream 1024p30 H.264 gvaclassify EfficientNet-B0 FP16 (iGPU) MFG-WPC-003 Weld Porosity Classification - multi-stream 1024p30 H.264 gvaclassify EfficientNet-B0 FP16 (dGPU) <p>Run this profile: <pre><code>esq run --profile profile.vertical.manufacturing\n</code></pre></p>"},{"location":"getting-started/suites/#metro","title":"Metro","text":"<p>Profile: <code>profile.vertical.metro</code></p> <p>Test Cases:</p> Test ID Test Case METRO-PROXY-001 LPR Pipeline (Multi-Devices) METRO-PROXY-002 Smart NVR (iGPU) METRO-PROXY-003 Smart NVR (dGPU) METRO-PROXY-004 Headed Visual AI Proxy Pipeline (iGPU) METRO-PROXY-005 Headed Visual AI Proxy Pipeline (dGPU) METRO-PROXY-006 VSaaS Visual AI Proxy Pipeline (iGPU) METRO-PROXY-007 VSaaS Visual AI Proxy Pipeline (dGPU) <p>Run this profile: <pre><code>esq run --profile profile.vertical.metro\n</code></pre></p>"},{"location":"getting-started/suites/#retail","title":"Retail","text":""},{"location":"getting-started/suites/#automated-self-checkout","title":"Automated Self Checkout","text":"<p>Profile: <code>profile.vertical.retail-asc</code></p> <p>Test Cases:</p> Test ID Test Case RTL-ASC-001 Automated Self Checkout - multi-stream 1920p15 H.264 gvadetect YOLO11n INT8 (CPU) RTL-ASC-002 Automated Self Checkout - multi-stream 1920p15 H.264 gvadetect YOLO11n INT8 (iGPU) RTL-ASC-003 Automated Self Checkout - multi-stream 1920p15 H.264 gvadetect YOLO11n INT8 (dGPU) RTL-ASC-004 Automated Self Checkout - multi-stream 1920p15 H.264 gvadetect YOLO11n INT8 (NPU) <p>Run this profile: <pre><code>esq run --profile profile.vertical.retail-asc\n</code></pre></p>"},{"location":"getting-started/suites/#loss-prevention","title":"Loss Prevention","text":"<p>Profile: <code>profile.vertical.retail-lp</code></p> <p>Test Cases:</p> Test ID Test Case RTL-LPP-001 Loss Prevention - multi-stream 1080p15 Items-in-Basket H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 (iGPU) RTL-LPP-002 Loss Prevention - multi-stream 1080p15 Hidden-Items-Product-Switching H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 (iGPU) RTL-LPP-003 Loss Prevention - multi-stream 1080p15 Fake-Scan-Detection H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 (iGPU) RTL-LPP-004 Loss Prevention - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 (iGPU) <p>Run this profile: <pre><code>esq run --profile profile.vertical.retail-lp\n</code></pre></p>"},{"location":"getting-started/suites/#horizontal","title":"Horizontal","text":""},{"location":"getting-started/suites/#generative-ai","title":"Generative AI","text":"<p>Profile: <code>profile.suite.ai.gen</code></p> Test Cases (click to expand) Test ID Test Case GEN-LLM-001 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-1.5B INT4 (CPU) GEN-LLM-002 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-1.5B INT4 (iGPU) GEN-LLM-003 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-1.5B INT4 (dGPU) GEN-LLM-004 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-1.5B INT4 (Hetero dGPU) GEN-LLM-005 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-1.5B INT4 (NPU) GEN-LLM-006 LLM Serving Benchmark - Phi-4-mini-reasoning 3.8B INT4 (CPU) GEN-LLM-007 LLM Serving Benchmark - Phi-4-mini-reasoning 3.8B INT4 (iGPU) GEN-LLM-008 LLM Serving Benchmark - Phi-4-mini-reasoning 3.8B INT4 (dGPU) GEN-LLM-009 LLM Serving Benchmark - Phi-4-mini-reasoning 3.8B INT4 (Hetero dGPU) GEN-LLM-010 LLM Serving Benchmark - Phi-4-mini-reasoning 3.8B INT4 (NPU) GEN-LLM-011 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-7B INT4 (CPU) GEN-LLM-012 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-7B INT4 (iGPU) GEN-LLM-013 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-7B INT4 (dGPU) GEN-LLM-014 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-7B INT4 (Hetero dGPU) GEN-LLM-015 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-7B INT4 (NPU) GEN-LLM-016 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-14B INT4 (CPU) GEN-LLM-017 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-14B INT4 (iGPU) GEN-LLM-018 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-14B INT4 (dGPU) GEN-LLM-019 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-14B INT4 (Hetero dGPU) GEN-LLM-020 LLM Serving Benchmark - DeepSeek-R1-Distill-Qwen-14B INT4 (NPU) GEN-LLM-021 LLM Serving Benchmark - Qwen3-32B INT4 (CPU) GEN-LLM-022 LLM Serving Benchmark - Qwen3-32B INT4 (iGPU) GEN-LLM-023 LLM Serving Benchmark - Qwen3-32B INT4 (dGPU) GEN-LLM-024 LLM Serving Benchmark - Qwen3-32B INT4 (Hetero dGPU) GEN-LLM-025 LLM Serving Benchmark - Qwen3-32B INT4 (NPU) GEN-LLM-026 LLM Serving Benchmark - DeepSeek-R1-Distill-Llama-70B INT4 (CPU) GEN-LLM-027 LLM Serving Benchmark - DeepSeek-R1-Distill-Llama-70B INT4 (iGPU) GEN-LLM-028 LLM Serving Benchmark - DeepSeek-R1-Distill-Llama-70B INT4 (dGPU) GEN-LLM-029 LLM Serving Benchmark - DeepSeek-R1-Distill-Llama-70B INT4 (Hetero dGPU) GEN-LLM-030 LLM Serving Benchmark - DeepSeek-R1-Distill-Llama-70B INT4 (NPU) <p></p> <p>Run this profile: <pre><code>esq run --profile profile.suite.ai.gen\n</code></pre></p>"},{"location":"getting-started/suites/#vision-ai","title":"Vision AI","text":""},{"location":"getting-started/suites/#dl-streamer-analysis-multi-stream-pipelines-with-multiple-ai-stages","title":"DL Streamer Analysis - Multi-Stream Pipelines With Multiple AI Stages","text":"<p>Profile: <code>profile.suite.ai.vision-light</code></p> <p>Test Cases:</p> Test ID Test Case VSN-LGT-001 DL Streamer Analysis - multi-stream 1080p30 H.265 gvadetect YOLO11n INT8 gvatrack gvaclassify ResNet50 INT8 VSN-LGT-002 DL Streamer Analysis - multi-stream 1080p30 H.265 gvadetect YOLO11n INT8 gvatrack gvaclassify ResNet50 INT8 (CPU) VSN-LGT-003 DL Streamer Analysis - multi-stream 1080p30 H.265 gvadetect YOLO11n INT8 gvatrack gvaclassify ResNet50 INT8 (iGPU) VSN-LGT-004 DL Streamer Analysis - multi-stream 1080p30 H.265 gvadetect YOLO11n INT8 gvatrack gvaclassify ResNet50 INT8 (dGPU) VSN-LGT-005 DL Streamer Analysis - multi-stream 1080p30 H.265 gvadetect YOLO11n INT8 gvatrack gvaclassify ResNet50 INT8 (NPU) <p>Run this profile: <pre><code>esq run --profile profile.suite.ai.vision-light\n</code></pre></p>"},{"location":"getting-started/suites/#verified-reference-blueprints","title":"Verified Reference Blueprints","text":"<p>Profile: <code>profile.suite.ai.vision-vrb</code></p> <p>Test Cases:</p> Test ID Test Case VSN-VRB-001 DL Streamer Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 VSN-VRB-002 DL Streamer Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 (CPU) VSN-VRB-003 DL Streamer Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 (iGPU) VSN-VRB-004 DL Streamer Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 (dGPU) VSN-VRB-005 DL Streamer Analysis - multi-stream 1080p15 H.264 gvadetect YOLO11n INT8 gvatrack gvaclassify EfficientNet-B0 INT8 (NPU) <p>Run this profile: <pre><code>esq run --profile profile.suite.ai.vision-vrb\n</code></pre></p>"},{"location":"getting-started/suites/#openvino","title":"OpenVINO","text":"<p>Profile: <code>profile.suite.ai.vision-ov</code></p> Test Cases (click to expand) Test ID Test Case VSN-OBM-001 OpenVINO Benchmark - resnet-50-tf INT8 (iGPU) VSN-OBM-002 OpenVINO Benchmark - resnet-50-tf INT8 (dGPU) VSN-OBM-003 OpenVINO Benchmark - resnet-50-tf INT8 (NPU) VSN-OBM-004 OpenVINO Benchmark - efficientnet-b0 INT8 (iGPU) VSN-OBM-005 OpenVINO Benchmark - efficientnet-b0 INT8 (dGPU) VSN-OBM-006 OpenVINO Benchmark - efficientnet-b0 INT8 (NPU) VSN-OBM-007 OpenVINO Benchmark - ssdlite_mobilenet_v2 INT8 (iGPU) VSN-OBM-008 OpenVINO Benchmark - ssdlite_mobilenet_v2 INT8 (dGPU) VSN-OBM-009 OpenVINO Benchmark - ssdlite_mobilenet_v2 INT8 (NPU) VSN-OBM-010 OpenVINO Benchmark - mobilenet-v2-pytorch INT8 (iGPU) VSN-OBM-011 OpenVINO Benchmark - mobilenet-v2-pytorch INT8 (dGPU) VSN-OBM-012 OpenVINO Benchmark - mobilenet-v2-pytorch INT8 (NPU) VSN-OBM-013 OpenVINO Benchmark - yolo-v5s INT8 (iGPU) VSN-OBM-014 OpenVINO Benchmark - yolo-v5s INT8 (dGPU) VSN-OBM-015 OpenVINO Benchmark - yolo-v5s INT8 (NPU) VSN-OBM-016 OpenVINO Benchmark - yolo-v8s INT8 (iGPU) VSN-OBM-017 OpenVINO Benchmark - yolo-v8s INT8 (dGPU) VSN-OBM-018 OpenVINO Benchmark - clip-vit-base-patch16 INT8 (iGPU) VSN-OBM-019 OpenVINO Benchmark - clip-vit-base-patch16 INT8 (dGPU) <p></p> <p>Run this profile: <pre><code>esq run --profile profile.suite.ai.vision-ov\n</code></pre></p>"},{"location":"getting-started/suites/#system-gpu-openvino","title":"System GPU - OpenVINO","text":"<p>Profile: <code>profile.suite.system.gpu-ov</code></p> <p>Test Cases:</p> Test ID Test Case GPU-OBM-001 AI GPU Frequency Measure - OV Benchmark yolo-v5s FP16 (iGPU) GPU-OBM-002 AI GPU Frequency Measure - OV Benchmark yolo-v5s FP16 (dGPU) <p>Run this profile: <pre><code>esq run --profile profile.suite.system.gpu-ov\n</code></pre></p>"},{"location":"getting-started/suites/#system-memory-stream","title":"System Memory - STREAM","text":"<p>Profile: <code>profile.suite.system.memory-stream</code></p> <p>Test Cases:</p> Test ID Test Case MEM-STR-001 STREAM Memory Benchmark - Copy MEM-STR-002 STREAM Memory Benchmark - Scale MEM-STR-003 STREAM Memory Benchmark - Add MEM-STR-004 STREAM Memory Benchmark - Triad <p>Run this profile: <pre><code>esq run --profile profile.suite.system.memory-stream\n</code></pre></p>"},{"location":"getting-started/suites/#media-performance","title":"Media Performance","text":"<p>Profile: <code>profile.suite.media.performance-pipelines</code></p> Test Cases (click to expand) Test ID Test Case MDA-DEC-001 Media Decode 4Mbps H.264 1080p@30 (iGPU) MDA-DEC-002 Media Decode 16Mbps H.264 4k@30 (iGPU) MDA-DEC-003 Media Decode 4Mbps H.264 1080p@30 (dGPU) MDA-DEC-004 Media Decode 16Mbps H.264 4k@30 (dGPU) MDA-DEC-005 Media Decode 2Mbps H.265 1080p@30 (iGPU) MDA-DEC-006 Media Decode 8Mbps H.265 4k@30 (iGPU) MDA-DEC-007 Media Decode 2Mbps H.265 1080p@30 (dGPU) MDA-DEC-008 Media Decode 8Mbps H.265 4k@30 (dGPU) MDA-COMP-001 Media Decode + Compose 4Mbps H.264 1080p@30 (iGPU) MDA-COMP-002 Media Decode + Compose 16Mbps H.264 4k@30 (iGPU) MDA-COMP-003 Media Decode + Compose 2Mbps H.265 1080p@30 (iGPU) MDA-COMP-004 Media Decode + Compose 8Mbps H.265 4k@30 (iGPU) MDA-COMP-005 Media Decode + Compose 4Mbps H.264 1080p@30 (dGPU) MDA-COMP-006 Media Decode + Compose 16Mbps H.264 4k@30 (dGPU) MDA-COMP-007 Media Decode + Compose 2Mbps H.265 1080p@30 (dGPU) MDA-COMP-008 Media Decode + Compose 8Mbps H.265 4k@30 (dGPU) MDA-ENC-001 Media Encode 4Mbps H.264 1080p@30 (iGPU) MDA-ENC-002 Media Encode 16Mbps H.264 4k@30 (iGPU) MDA-ENC-003 Media Encode 4Mbps H.264 1080p@30 (dGPU) MDA-ENC-004 Media Encode 16Mbps H.264 4k@30 (dGPU) MDA-ENC-005 Media Encode 2Mbps H.265 1080p@30 (iGPU) MDA-ENC-006 Media Encode 8Mbps H.265 4k@30 (iGPU) MDA-ENC-007 Media Encode 2Mbps H.265 1080p@30 (dGPU) MDA-ENC-008 Media Encode 8Mbps H.265 4k@30 (dGPU) <p></p> <p>Run this profile: <pre><code>esq run --profile profile.suite.media.performance-pipelines\n</code></pre></p>"},{"location":"guides/","title":"Guides","text":"<p>Comprehensive guides for using and developing with Intel\u00ae ESQ.</p>"},{"location":"guides/#available-guides","title":"Available Guides","text":"<ul> <li>Developer Guide - Complete guide for integrating pytest tests into the ESQ framework</li> <li>Developer Quick Reference - Cheat sheet with templates and common commands</li> <li>Optimization Guide - Performance optimization techniques</li> <li>Troubleshooting Guide - Common issues and solutions</li> </ul>"},{"location":"guides/developer-quick-reference/","title":"Developer Quick Reference","text":"<p>Quick reference guide for common tasks when developing tests in the Intel\u00ae ESQ framework.</p> <p>Documentation in progress.</p>"},{"location":"guides/developer/","title":"Developer Guide","text":"<p>Comprehensive guide for developers integrating their own pytest tests into the Intel\u00ae ESQ framework.</p> <p>Quick Reference: Looking for a quick cheat sheet? See the Developer Quick Reference for templates and common commands.</p>"},{"location":"guides/developer/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Framework Architecture</li> <li>Creating Your First Test</li> <li>Configuration System</li> <li>Available Fixtures</li> <li>System Requirements Flags</li> <li>Test Execution Pattern</li> <li>Working with Results and Metrics</li> <li>KPI Validation</li> <li>Asset Management</li> <li>Best Practices</li> <li>Advanced Topics</li> </ul>"},{"location":"guides/developer/#overview","title":"Overview","text":"<p>The Intel\u00ae ESQ framework provides a comprehensive pytest-based testing infrastructure with:</p> <ul> <li>Automatic test parameterization from YAML configuration files</li> <li>Built-in fixtures for caching, validation, and reporting</li> <li>System requirement validation with reusable flags</li> <li>KPI-based test validation with flexible configuration</li> <li>Asset management for models, videos, and files</li> <li>Allure reporting with rich visualizations</li> <li>Docker integration for containerized tests</li> </ul> <p>This guide will help you integrate your own tests into this framework and leverage its powerful features.</p>"},{"location":"guides/developer/#framework-architecture","title":"Framework Architecture","text":""},{"location":"guides/developer/#dual-package-structure","title":"Dual-Package Structure","text":"<pre><code>src/\n\u251c\u2500\u2500 sysagent/           # Core framework (reusable infrastructure)\n\u2502   \u251c\u2500\u2500 cli.py          # Main CLI entry point\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 plugins/    # Pytest fixtures and hooks\n\u2502   \u2502   \u251c\u2500\u2500 core/       # Result, Metrics, Cache classes\n\u2502   \u2502   \u251c\u2500\u2500 testing/    # System validation utilities\n\u2502   \u2502   \u2514\u2500\u2500 config/     # Configuration loaders\n\u2502   \u2514\u2500\u2500 suites/         # Core test suites (examples)\n\u2502\n\u2514\u2500\u2500 esq/                # ESQ-specific extensions\n    \u251c\u2500\u2500 suites/         # Domain-specific test suites\n    \u2502   \u251c\u2500\u2500 ai/         # AI tests (vision, audio, gen)\n    \u2502   \u251c\u2500\u2500 media/      # Media processing tests\n    \u2502   \u251c\u2500\u2500 system/     # System-level tests\n    \u2502   \u2514\u2500\u2500 vertical/   # Vertical-specific tests\n    \u2514\u2500\u2500 configs/\n        \u2514\u2500\u2500 profiles/   # Test profiles (qualifications, suites, verticals)\n</code></pre>"},{"location":"guides/developer/#test-discovery-flow","title":"Test Discovery Flow","text":"<pre><code>Profile YAML \u2192 Consolidator \u2192 Pytest Plugin \u2192 Test Function\n     \u2193              \u2193              \u2193               \u2193\n  params       merge with     filter by      configs fixture\n               defaults    function name    (all merged params)\n</code></pre>"},{"location":"guides/developer/#creating-your-first-test","title":"Creating Your First Test","text":""},{"location":"guides/developer/#step-1-create-test-file","title":"Step 1: Create Test File","text":"<p>Create a test file in an appropriate suite directory:</p> <pre><code>src/esq/suites/\n\u2514\u2500\u2500 my_domain/\n    \u2514\u2500\u2500 my_feature/\n        \u251c\u2500\u2500 config.yml          # KPI and test configurations\n        \u2514\u2500\u2500 test_my_feature.py  # Your test implementation\n</code></pre>"},{"location":"guides/developer/#step-2-basic-test-structure","title":"Step 2: Basic Test Structure","text":"<pre><code># src/esq/suites/my_domain/my_feature/test_my_feature.py\n# Copyright (C) 2025 Intel Corporation\n# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nfrom sysagent.utils.core import Result, Metrics\n\nlogger = logging.getLogger(__name__)\n\n\ndef test_my_feature(\n    request,\n    configs,\n    cached_result,\n    cache_result,\n    get_kpi_config,\n    validate_test_results,\n    summarize_test_results,\n    validate_system_requirements_from_configs,\n    execute_test_with_cache,\n    prepare_test,\n):\n    \"\"\"\n    My custom feature test.\n\n    This test demonstrates the standard 7-step pattern for ESQ tests.\n    \"\"\"\n    # Step 1: Extract parameters from configs\n    test_name = request.node.name.split(\"[\")[0]\n    test_id = configs.get(\"test_id\", test_name)\n    test_display_name = configs.get(\"display_name\", test_name)\n    timeout = configs.get(\"timeout\", 300)\n    devices = configs.get(\"devices\", [\"cpu\"])\n\n    logger.info(f\"Starting test: {test_display_name}\")\n\n    # Step 2: Validate system requirements\n    validate_system_requirements_from_configs(configs)\n\n    # Step 3: Prepare assets/dependencies\n    def prepare_assets():\n        \"\"\"Prepare test assets and dependencies.\"\"\"\n        # Download models, prepare data, etc.\n        return Result(\n            name=f\"{test_id} - Asset Preparation\",\n            metadata={\"status\": \"completed\"}\n        )\n\n    prepare_test(\n        test_name=test_name,\n        prepare_func=prepare_assets,\n        configs=configs,\n        name=\"Assets\"\n    )\n\n    # Step 4: Execute test logic (with caching)\n    def execute_logic():\n        \"\"\"Execute the main test logic.\"\"\"\n        results = Result(name=f\"{test_id} - {test_display_name}\")\n\n        # Your test implementation here\n        # Example: Run inference, process data, etc.\n        inference_time = 0.123  # seconds\n        accuracy = 0.95  # 95%\n\n        # Add metrics to results\n        results.metrics[\"inference_time\"] = Metrics(\n            value=inference_time,\n            unit=\"seconds\"\n        )\n        results.metrics[\"accuracy\"] = Metrics(\n            value=accuracy,\n            unit=\"percentage\"\n        )\n\n        # Add parameters\n        results.parameters[\"Test ID\"] = test_id\n        results.parameters[\"Display Name\"] = test_display_name\n        results.parameters[\"Devices\"] = \", \".join(devices)\n\n        # Update timestamps\n        results.update_timestamps()\n\n        return results\n\n    results = execute_test_with_cache(\n        cached_result=cached_result,\n        cache_result=cache_result,\n        run_test_func=execute_logic,\n        test_name=test_name,\n        configs=configs\n    )\n\n    # Step 5: Validate results (if qualification profile)\n    validation_results = validate_test_results(\n        results=results,\n        configs=configs,\n        get_kpi_config=get_kpi_config,\n        test_name=test_name\n    )\n\n    # Step 6: Generate summary\n    summarize_test_results(\n        results=results,\n        test_name=test_name,\n        configs=configs,\n        get_kpi_config=get_kpi_config\n    )\n</code></pre>"},{"location":"guides/developer/#step-3-create-configuration-file","title":"Step 3: Create Configuration File","text":"<pre><code># src/esq/suites/my_domain/my_feature/config.yml\n# Copyright (C) 2025 Intel Corporation\n# SPDX-License-Identifier: Apache-2.0\n\n# KPI definitions for this test suite\nkpi:\n  inference_time:\n    name: \"Inference Time\"\n    type: \"numeric\"\n    validation:\n      operator: \"lte\"  # less than or equal\n      reference: 0.5   # 500ms max\n      enabled: true\n    unit: \"seconds\"\n    severity: \"major\"\n    description: \"Time taken for model inference\"\n    default_value: 999.0\n\n  accuracy:\n    name: \"Model Accuracy\"\n    type: \"numeric\"\n    validation:\n      operator: \"gte\"  # greater than or equal\n      reference: 0.90  # 90% minimum\n      enabled: true\n    unit: \"percentage\"\n    severity: \"critical\"\n    description: \"Model prediction accuracy\"\n    default_value: 0.0\n\n# Test configurations\ntests:\n  test_my_feature:\n    params:\n      - test_id: \"MF-001\"\n        display_name: \"My Feature Test - CPU\"\n        devices: [cpu]\n        timeout: 300\n        kpi_refs:\n          - inference_time\n          - accuracy\n        requirements:\n          cpu_min_cores: 4\n          memory_min_gb: 8.0\n          docker_required: false\n\n      - test_id: \"MF-002\"\n        display_name: \"My Feature Test - GPU\"\n        devices: [igpu]\n        timeout: 300\n        kpi_refs:\n          - inference_time\n          - accuracy\n        requirements:\n          igpu_required: true\n          memory_min_gb: 8.0\n</code></pre>"},{"location":"guides/developer/#step-4-create-a-profile","title":"Step 4: Create a Profile","text":"<pre><code># src/esq/configs/profiles/suites/my_feature.yml\n# Copyright (C) 2025 Intel Corporation\n# SPDX-License-Identifier: Apache-2.0\n\nname: \"profile.suite.my_feature\"\ndescription: \"My Feature Test Suite\"\nversion: \"1.0.0\"\nparams:\n  labels:\n    profile_display_name: \"My Feature\"\n    group: \"custom.my_feature\"\n    type: \"suite\"\n  requirements:\n    cpu_min_cores: 4\n    memory_min_gb: 8.0\n    storage_min_gb: 5.0\n    os_type:\n      - \"linux\"\n    docker_required: false\n\nsuites:\n  - name: \"my_domain\"\n    sub_suites:\n      - name: \"my_feature\"\n        tests:\n          test_my_feature:\n            params:\n              - test_id: \"MF-001\"\n                display_name: \"My Feature Test - CPU\"\n                devices: [cpu]\n\n              - test_id: \"MF-002\"\n                display_name: \"My Feature Test - GPU\"\n                devices: [igpu]\n                requirements:\n                  igpu_required: true\n</code></pre>"},{"location":"guides/developer/#step-5-run-your-test","title":"Step 5: Run Your Test","text":"<pre><code># List available profiles\nesq list\n\n# Run your profile\nesq -v run --profile profile.suite.my_feature\n\n# Run specific test with filter\nesq -d run --profile profile.suite.my_feature --filter test_id=MF-001\n\n# Run without cache\nesq -d run -nc --profile profile.suite.my_feature\n</code></pre>"},{"location":"guides/developer/#configuration-system","title":"Configuration System","text":""},{"location":"guides/developer/#profile-structure","title":"Profile Structure","text":"<p>Profiles define test execution plans with parameters and requirements:</p> <pre><code>name: \"profile.suite.example\"\ndescription: \"Example test suite\"\nversion: \"1.0.0\"\n\nparams:\n  labels:\n    profile_display_name: \"Example\"\n    group: \"example.group\"\n    type: \"suite\"  # or \"qualification\" or \"vertical\"\n\n  requirements:\n    # Hardware requirements\n    cpu_min_cores: 4\n    memory_min_gb: 8.0\n    storage_min_gb: 10.0\n\n    # Software requirements\n    os_type: [\"linux\"]\n    docker_required: true\n\n    # Device requirements\n    igpu_required: false\n    dgpu_required: false\n    npu_required: false\n\nsuites:\n  - name: \"suite_name\"\n    sub_suites:\n      - name: \"sub_suite_name\"\n        tests:\n          test_function_name:\n            params:\n              - test_id: \"EX-001\"\n                display_name: \"Example Test\"\n                devices: [cpu, igpu]\n                timeout: 300\n                kpi_refs:\n                  - example_kpi\n</code></pre>"},{"location":"guides/developer/#profile-to-test-mapping","title":"Profile to Test Mapping","text":"<p>The framework automatically maps profile configuration to test files:</p> <pre><code>Profile YAML                          Test File\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nsuites:\n  - name: \"ai\"                    \u2192   src/esq/suites/ai/\n    sub_suites:\n      - name: \"vision\"            \u2192   src/esq/suites/ai/vision/\n        tests:\n          test_dlstreamer:        \u2192   test_dlstreamer.py\n            params:\n              - test_id: \"VSN-001\"\n                devices: [cpu]\n</code></pre>"},{"location":"guides/developer/#test-configuration-configyml","title":"Test Configuration (config.yml)","text":"<p>Each test directory can have a <code>config.yml</code> defining KPIs and default parameters:</p> <pre><code># KPI definitions\nkpi:\n  metric_name:\n    name: \"Human Readable Name\"\n    type: \"numeric\"  # or \"string\", \"boolean\", \"list\"\n    validation:\n      operator: \"gte\"  # gte, lte, eq, ne, gt, lt\n      reference: 100.0\n      enabled: true\n    unit: \"unit_name\"\n    severity: \"critical\"  # critical, major, normal, minor\n    description: \"KPI description\"\n    default_value: 0.0\n\n# Test-specific parameters\ntests:\n  test_function_name:\n    params:\n      - test_id: \"T001\"\n        display_name: \"Test Name\"\n        kpi_refs:\n          - metric_name\n</code></pre>"},{"location":"guides/developer/#available-fixtures","title":"Available Fixtures","text":"<p>The framework provides comprehensive pytest fixtures automatically available to all tests.</p>"},{"location":"guides/developer/#core-fixtures","title":"Core Fixtures","text":""},{"location":"guides/developer/#request","title":"<code>request</code>","text":"<ul> <li>Type: pytest.FixtureRequest</li> <li>Scope: function</li> <li>Description: Standard pytest request object for accessing test metadata</li> <li>Usage:   <pre><code>test_name = request.node.name.split(\"[\")[0]\n</code></pre></li> </ul>"},{"location":"guides/developer/#configs","title":"<code>configs</code>","text":"<ul> <li>Type: Dict[str, Any]</li> <li>Scope: function</li> <li>Description: Test configuration parameters merged from profile and config.yml</li> <li>Usage:   <pre><code>test_id = configs.get(\"test_id\", \"T0000\")\ndevices = configs.get(\"devices\", [\"cpu\"])\ntimeout = configs.get(\"timeout\", 300)\n</code></pre></li> </ul>"},{"location":"guides/developer/#cache-fixtures","title":"Cache Fixtures","text":""},{"location":"guides/developer/#cached_result","title":"<code>cached_result</code>","text":"<ul> <li>Type: Callable[[Optional[Dict]], Optional[Union[Result, Dict]]]</li> <li>Scope: function</li> <li>Description: Retrieves cached test results if available</li> <li>Usage:   <pre><code>cached_data = cached_result(cache_configs={\"key\": \"value\"})\nif cached_data:\n    return cached_data\n</code></pre></li> </ul>"},{"location":"guides/developer/#cache_result","title":"<code>cache_result</code>","text":"<ul> <li>Type: Callable[[Union[Result, Dict], Optional[Dict]], None]</li> <li>Scope: function</li> <li>Description: Stores test results in cache</li> <li>Usage:   <pre><code>cache_result(results, cache_configs={\"key\": \"value\"})\n</code></pre></li> </ul>"},{"location":"guides/developer/#validation-fixtures","title":"Validation Fixtures","text":""},{"location":"guides/developer/#validate_system_requirements_from_configs","title":"<code>validate_system_requirements_from_configs</code>","text":"<ul> <li>Type: Callable[[Dict[str, Any]], None]</li> <li>Scope: function</li> <li>Description: Validates system meets test requirements from configs</li> <li>Usage:   <pre><code>validate_system_requirements_from_configs(configs)\n# Raises pytest.skip if requirements not met\n</code></pre></li> </ul>"},{"location":"guides/developer/#validate_test_results","title":"<code>validate_test_results</code>","text":"<ul> <li>Type: Callable[..., Dict[str, Any]]</li> <li>Scope: function</li> <li>Description: Validates test results against KPI thresholds</li> <li>Parameters:</li> <li><code>results</code>: Result object or dict</li> <li><code>configs</code>: Test configuration</li> <li><code>get_kpi_config</code>: KPI config retrieval function</li> <li><code>test_name</code>: Test identifier</li> <li><code>mode</code>: Validation mode (\"all\" or \"any\")</li> <li>Usage:   <pre><code>validation_results = validate_test_results(\n    results=results,\n    configs=configs,\n    get_kpi_config=get_kpi_config,\n    test_name=test_name\n)\n# Returns: {\"passed\": bool, \"validations\": {...}, \"skipped\": bool}\n</code></pre></li> </ul>"},{"location":"guides/developer/#execution-fixtures","title":"Execution Fixtures","text":""},{"location":"guides/developer/#execute_test_with_cache","title":"<code>execute_test_with_cache</code>","text":"<ul> <li>Type: Callable[..., Union[Result, Dict]]</li> <li>Scope: function</li> <li>Description: Executes test with automatic caching support</li> <li>Parameters:</li> <li><code>cached_result</code>: Cached result fixture</li> <li><code>cache_result</code>: Cache result fixture</li> <li><code>run_test_func</code>: Function to execute test logic</li> <li><code>test_name</code>: Test identifier</li> <li><code>configs</code>: Test configuration</li> <li><code>cache_configs</code>: Optional cache-specific configs</li> <li><code>name</code>: Step name (default: \"Analysis\")</li> <li>Usage:   <pre><code>results = execute_test_with_cache(\n    cached_result=cached_result,\n    cache_result=cache_result,\n    run_test_func=lambda: execute_my_test(),\n    test_name=test_name,\n    configs=configs\n)\n</code></pre></li> </ul>"},{"location":"guides/developer/#prepare_test","title":"<code>prepare_test</code>","text":"<ul> <li>Type: Callable[..., Any]</li> <li>Scope: function</li> <li>Description: Prepares test assets with progress tracking</li> <li>Parameters:</li> <li><code>test_name</code>: Test identifier</li> <li><code>prepare_func</code>: Function to execute preparation</li> <li><code>configs</code>: Test configuration</li> <li><code>name</code>: Step name (default: \"Preparation\")</li> <li>Usage:   <pre><code>prepare_test(\n    test_name=test_name,\n    prepare_func=lambda: download_models(),\n    configs=configs,\n    name=\"Assets\"\n)\n</code></pre></li> </ul>"},{"location":"guides/developer/#reporting-fixtures","title":"Reporting Fixtures","text":""},{"location":"guides/developer/#summarize_test_results","title":"<code>summarize_test_results</code>","text":"<ul> <li>Type: Callable[..., None]</li> <li>Scope: function</li> <li>Description: Generates test result summary with Allure attachments</li> <li>Parameters:</li> <li><code>results</code>: Result object</li> <li><code>test_name</code>: Test identifier</li> <li><code>configs</code>: Test configuration (optional)</li> <li><code>get_kpi_config</code>: KPI config function (optional)</li> <li><code>iteration_data</code>: Iteration-level data (optional)</li> <li><code>enable_visualizations</code>: Enable charts (default: False)</li> <li>Usage:   <pre><code>summarize_test_results(\n    results=results,\n    test_name=test_name,\n    configs=configs,\n    get_kpi_config=get_kpi_config\n)\n</code></pre></li> </ul>"},{"location":"guides/developer/#suite-fixtures","title":"Suite Fixtures","text":""},{"location":"guides/developer/#suite_configs","title":"<code>suite_configs</code>","text":"<ul> <li>Type: Dict[str, Any]</li> <li>Scope: function</li> <li>Description: Loads suite-level configuration from config.yml</li> <li>Usage:   <pre><code>kpis = suite_configs.get(\"kpi\", {})\n</code></pre></li> </ul>"},{"location":"guides/developer/#get_kpi_config","title":"<code>get_kpi_config</code>","text":"<ul> <li>Type: Callable[[str], Optional[Dict[str, Any]]]</li> <li>Scope: function</li> <li>Description: Retrieves KPI configuration by name</li> <li>Usage:   <pre><code>kpi_config = get_kpi_config(\"inference_time\")\nif kpi_config:\n    reference = kpi_config[\"validation\"][\"reference\"]\n</code></pre></li> </ul>"},{"location":"guides/developer/#system-requirements-flags","title":"System Requirements Flags","text":"<p>The framework provides reusable requirement validation flags. Add these to your profile or test parameters under the <code>requirements</code> key.</p>"},{"location":"guides/developer/#hardware-requirements","title":"Hardware Requirements","text":""},{"location":"guides/developer/#cpu-requirements","title":"CPU Requirements","text":"<pre><code>requirements:\n  # Minimum CPU cores\n  cpu_min_cores: 4\n\n  # Minimum CPU threads\n  cpu_min_threads: 8\n\n  # CPU brand requirements\n  cpu_xeon_required: true     # Requires Intel\u00ae Xeon\u00ae processor\n  cpu_core_required: true     # Requires Intel\u00ae Core\u2122 processor (includes Ultra Desktop)\n  cpu_ultra_required: true    # Requires Intel\u00ae Ultra processor\n\n  # CPU socket count\n  cpu_min_sockets: 1\n  cpu_max_sockets: 2\n</code></pre>"},{"location":"guides/developer/#memory-requirements","title":"Memory Requirements","text":"<pre><code>requirements:\n  # Minimum total memory\n  memory_min_gb: 8.0\n\n  # Maximum memory (for constrained tests)\n  memory_max_gb: 32.0\n</code></pre>"},{"location":"guides/developer/#storage-requirements","title":"Storage Requirements","text":"<pre><code>requirements:\n  # Total storage capacity\n  storage_total_min_gb: 64.0\n\n  # Free storage for test execution\n  storage_min_gb: 10.0\n</code></pre>"},{"location":"guides/developer/#device-requirements","title":"Device Requirements","text":"<pre><code>requirements:\n  # Integrated GPU (iGPU)\n  igpu_required: true\n  igpu_min_count: 1\n  igpu_max_count: 1\n\n  # Discrete GPU (dGPU)\n  dgpu_required: true\n  dgpu_min_count: 1\n  dgpu_max_count: 4\n\n  # Neural Processing Unit (NPU)\n  npu_required: true\n  npu_min_count: 1\n  npu_max_count: 1\n</code></pre> <p>Device Detection Logic: - The framework automatically detects Intel devices using OpenVINO - Only Intel devices detected by OpenVINO are counted - Non-Intel devices are ignored for requirements validation - If Intel devices exist but aren't detected by OpenVINO, helpful error messages suggest driver installation</p>"},{"location":"guides/developer/#software-requirements","title":"Software Requirements","text":""},{"location":"guides/developer/#operating-system","title":"Operating System","text":"<pre><code>requirements:\n  # Supported OS types\n  os_type:\n    - \"linux\"\n    - \"windows\"\n\n  # Specific OS versions\n  os_version_min: \"20.04\"  # For Ubuntu\n  os_version_max: \"24.04\"\n</code></pre>"},{"location":"guides/developer/#docker-requirements","title":"Docker Requirements","text":"<pre><code>requirements:\n  # Docker daemon required\n  docker_required: true\n\n  # Minimum Docker version\n  docker_version_min: \"20.10.0\"\n</code></pre>"},{"location":"guides/developer/#software-dependencies","title":"Software Dependencies","text":"<pre><code>requirements:\n  # Python version\n  python_version_min: \"3.10\"\n\n  # Other software (validated via command presence)\n  software_required:\n    - name: \"git\"\n      command: \"git --version\"\n    - name: \"ffmpeg\"\n      command: \"ffmpeg -version\"\n</code></pre>"},{"location":"guides/developer/#example-complete-requirements-block","title":"Example: Complete Requirements Block","text":"<pre><code>requirements:\n  # Hardware\n  cpu_min_cores: 8\n  cpu_xeon_required: true\n  memory_min_gb: 16.0\n  storage_min_gb: 20.0\n  dgpu_required: true\n  dgpu_min_count: 2\n\n  # Software\n  os_type: [\"linux\"]\n  docker_required: true\n  python_version_min: \"3.10\"\n\n  # Dependencies\n  software_required:\n    - name: \"ffmpeg\"\n      command: \"ffmpeg -version\"\n</code></pre>"},{"location":"guides/developer/#using-requirements-in-tests","title":"Using Requirements in Tests","text":"<p>Requirements are automatically validated when you call:</p> <pre><code>validate_system_requirements_from_configs(configs)\n</code></pre> <p>This will: 1. Check all specified requirements against system capabilities 2. Skip the test if requirements aren't met (using <code>pytest.skip</code>) 3. Provide detailed error messages for failed requirements 4. Log fix suggestions for common issues</p> <p>Example Validation Output: <pre><code>Validation failed for profile: my-profile\n  CPU Requirements:\n    \u2717 CPU cores &gt;= 8 (Actual: 4 cores)\n  Device Requirements:\n    \u2717 dGPU required (Actual: 1 Intel dGPU found but not detected by OpenVINO)\n\nSuggestions:\n  - Upgrade to system with more CPU cores\n  - Install Intel GPU drivers: sudo apt install intel-gpu-tools\n</code></pre></p>"},{"location":"guides/developer/#test-execution-pattern","title":"Test Execution Pattern","text":"<p>The framework follows a standardized 7-step pattern for all tests:</p>"},{"location":"guides/developer/#standard-test-pattern","title":"Standard Test Pattern","text":"<pre><code>def test_example(\n    request,\n    configs,\n    cached_result,\n    cache_result,\n    get_kpi_config,\n    validate_test_results,\n    summarize_test_results,\n    validate_system_requirements_from_configs,\n    execute_test_with_cache,\n    prepare_test,\n):\n    \"\"\"Standard test pattern.\"\"\"\n\n    # ====================================================================\n    # STEP 1: Extract Parameters\n    # ====================================================================\n    test_name = request.node.name.split(\"[\")[0]\n    test_id = configs.get(\"test_id\", test_name)\n    test_display_name = configs.get(\"display_name\", test_name)\n    timeout = configs.get(\"timeout\", 300)\n    devices = configs.get(\"devices\", [\"cpu\"])\n\n    logger.info(f\"Starting test: {test_display_name}\")\n\n    # ====================================================================\n    # STEP 2: Validate System Requirements\n    # ====================================================================\n    # Automatically skips test if requirements not met\n    validate_system_requirements_from_configs(configs)\n\n    # ====================================================================\n    # STEP 3: Prepare Assets/Dependencies\n    # ====================================================================\n    def prepare_assets():\n        \"\"\"Prepare test assets.\"\"\"\n        # Download models, videos, files\n        # Build Docker images\n        # Set up test environment\n        return Result(\n            name=f\"{test_id} - Asset Preparation\",\n            metadata={\"status\": \"completed\"}\n        )\n\n    prepare_test(\n        test_name=test_name,\n        prepare_func=prepare_assets,\n        configs=configs,\n        name=\"Assets\"\n    )\n\n    # ====================================================================\n    # STEP 4: Execute Test Logic (with caching)\n    # ====================================================================\n    def execute_logic():\n        \"\"\"Main test logic.\"\"\"\n        results = Result(name=f\"{test_id} - {test_display_name}\")\n\n        # Your test implementation\n        # - Run benchmarks\n        # - Execute inference\n        # - Collect metrics\n\n        # Add metrics\n        results.metrics[\"throughput\"] = Metrics(\n            value=1234.5,\n            unit=\"ops/sec\"\n        )\n\n        # Add parameters\n        results.parameters[\"Test ID\"] = test_id\n        results.parameters[\"Devices\"] = \", \".join(devices)\n\n        # Update timestamps\n        results.update_timestamps()\n\n        return results\n\n    results = execute_test_with_cache(\n        cached_result=cached_result,\n        cache_result=cache_result,\n        run_test_func=execute_logic,\n        test_name=test_name,\n        configs=configs\n    )\n\n    # ====================================================================\n    # STEP 5: Validate Results Against KPIs\n    # ====================================================================\n    validation_results = validate_test_results(\n        results=results,\n        configs=configs,\n        get_kpi_config=get_kpi_config,\n        test_name=test_name\n    )\n\n    # ====================================================================\n    # STEP 6: Generate Summary\n    # ====================================================================\n    summarize_test_results(\n        results=results,\n        test_name=test_name,\n        configs=configs,\n        get_kpi_config=get_kpi_config\n    )\n\n    # ====================================================================\n    # STEP 7: Test Complete (implicit)\n    # ====================================================================\n    # Framework automatically:\n    # - Saves results to JSON\n    # - Generates Allure report\n    # - Caches results (if enabled)\n</code></pre>"},{"location":"guides/developer/#execution-flow","title":"Execution Flow","text":"<pre><code>1. Parameter Extraction\n   \u251c\u2500&gt; Read test_id, display_name, timeout, etc.\n   \u2514\u2500&gt; Configure logging\n\n2. System Validation\n   \u251c\u2500&gt; Check CPU/Memory/Storage\n   \u251c\u2500&gt; Check Device availability\n   \u251c\u2500&gt; Check Software dependencies\n   \u2514\u2500&gt; Skip test if requirements not met\n\n3. Asset Preparation\n   \u251c\u2500&gt; Download models/videos\n   \u251c\u2500&gt; Build Docker images\n   \u2514\u2500&gt; Set up test environment\n\n4. Test Execution (Cached)\n   \u251c\u2500&gt; Check cache for existing results\n   \u251c\u2500&gt; If cache hit: return cached results\n   \u2514\u2500&gt; If cache miss: execute test logic\n\n5. KPI Validation\n   \u251c\u2500&gt; Load KPI configurations\n   \u251c\u2500&gt; Compare results vs thresholds\n   \u2514\u2500&gt; Mark passed/failed/skipped\n\n6. Result Summarization\n   \u251c\u2500&gt; Generate JSON summary\n   \u251c\u2500&gt; Create Allure attachments\n   \u2514\u2500&gt; Generate visualizations\n\n7. Cleanup &amp; Reporting\n   \u251c\u2500&gt; Save results to file\n   \u251c\u2500&gt; Update Allure report\n   \u2514\u2500&gt; Clear temporary resources\n</code></pre>"},{"location":"guides/developer/#working-with-results-and-metrics","title":"Working with Results and Metrics","text":""},{"location":"guides/developer/#result-class","title":"Result Class","text":"<p>The <code>Result</code> dataclass provides structured test result handling:</p> <pre><code>from sysagent.utils.core import Result, Metrics\n\n# Create result object\nresults = Result(\n    name=\"T001 - My Test\",\n    parameters={\n        \"Test ID\": \"T001\",\n        \"Device\": \"CPU\",\n        \"Batch Size\": 8\n    },\n    metrics={\n        \"throughput\": Metrics(value=1234.5, unit=\"ops/sec\"),\n        \"latency\": Metrics(value=0.123, unit=\"seconds\")\n    },\n    metadata={\n        \"status\": \"completed\",\n        \"custom_field\": \"custom_value\"\n    }\n)\n\n# Update timestamps (auto-calculates duration)\nresults.update_timestamps()\n\n# Convert to dictionary for serialization\nresults_dict = results.to_dict()\n</code></pre>"},{"location":"guides/developer/#metrics-class","title":"Metrics Class","text":"<p>The <code>Metrics</code> dataclass structures metric data:</p> <pre><code>from sysagent.utils.core import Metrics\n\n# Create metric\nmetric = Metrics(\n    value=123.45,\n    unit=\"ms\",\n    is_key_metric=False\n)\n\n# Access metric properties\nprint(f\"Value: {metric.value} {metric.unit}\")\n</code></pre>"},{"location":"guides/developer/#adding-metrics-to-results","title":"Adding Metrics to Results","text":"<pre><code># Add single metric\nresults.metrics[\"accuracy\"] = Metrics(\n    value=0.95,\n    unit=\"percentage\"\n)\n\n# Add multiple metrics\nresults.metrics.update({\n    \"precision\": Metrics(value=0.93, unit=\"percentage\"),\n    \"recall\": Metrics(value=0.91, unit=\"percentage\"),\n    \"f1_score\": Metrics(value=0.92, unit=\"percentage\")\n})\n\n# Device-specific metrics\nfor device in [\"cpu\", \"igpu\", \"dgpu\"]:\n    results.metrics[f\"throughput_{device}\"] = Metrics(\n        value=get_throughput(device),\n        unit=\"fps\"\n    )\n</code></pre>"},{"location":"guides/developer/#key-metrics","title":"Key Metrics","text":"<p>Designate a primary metric for reporting:</p> <pre><code># Set key metric\nresults.set_key_metric(\"throughput\")\n\n# Get current key metric\nkey_metric_name = results.get_key_metric()\nif key_metric_name:\n    print(f\"Key metric: {key_metric_name}\")\n</code></pre>"},{"location":"guides/developer/#automatic-metadata","title":"Automatic Metadata","text":"<p>The <code>Result</code> class automatically includes:</p> <pre><code>{\n    \"created_at\": \"2025-12-29T10:00:00+00:00\",\n    \"updated_at\": \"2025-12-29T10:05:30+00:00\",\n    \"total_duration_seconds\": 330.0,\n    \"kpi_validation_status\": \"passed\"  # or \"failed\", \"skipped\"\n}\n</code></pre>"},{"location":"guides/developer/#complete-example","title":"Complete Example","text":"<pre><code>def execute_benchmark():\n    \"\"\"Execute benchmark and return results.\"\"\"\n    # Create result object\n    results = Result(name=f\"{test_id} - {test_display_name}\")\n\n    # Run benchmark\n    start_time = time.time()\n    throughput, latency = run_inference(model, data)\n    elapsed = time.time() - start_time\n\n    # Add metrics\n    results.metrics[\"throughput\"] = Metrics(\n        value=throughput,\n        unit=\"fps\",\n        is_key_metric=True\n    )\n    results.metrics[\"latency\"] = Metrics(\n        value=latency,\n        unit=\"ms\"\n    )\n    results.metrics[\"elapsed_time\"] = Metrics(\n        value=elapsed,\n        unit=\"seconds\"\n    )\n\n    # Add parameters\n    results.parameters[\"Model\"] = \"yolo11n\"\n    results.parameters[\"Precision\"] = \"INT8\"\n    results.parameters[\"Device\"] = \"GPU\"\n\n    # Add custom metadata\n    results.metadata[\"model_version\"] = \"1.0.0\"\n    results.metadata[\"batch_size\"] = 8\n\n    # Update timestamps\n    results.update_timestamps()\n\n    return results\n</code></pre>"},{"location":"guides/developer/#kpi-validation","title":"KPI Validation","text":""},{"location":"guides/developer/#defining-kpis","title":"Defining KPIs","text":"<p>KPIs are defined in <code>config.yml</code> files:</p> <pre><code>kpi:\n  inference_time:\n    name: \"Inference Time\"\n    type: \"numeric\"\n    validation:\n      operator: \"lte\"  # less than or equal\n      reference: 0.5   # 500ms threshold\n      enabled: true\n    unit: \"seconds\"\n    severity: \"major\"\n    description: \"Time taken for model inference\"\n    default_value: 999.0\n\n  accuracy:\n    name: \"Model Accuracy\"\n    type: \"numeric\"\n    validation:\n      operator: \"gte\"  # greater than or equal\n      reference: 0.90  # 90% minimum\n      enabled: true\n    unit: \"percentage\"\n    severity: \"critical\"\n    description: \"Model prediction accuracy\"\n    default_value: 0.0\n\n  status:\n    name: \"Test Status\"\n    type: \"string\"\n    validation:\n      operator: \"eq\"  # equals\n      reference: \"success\"\n      enabled: true\n    unit: \"\"\n    severity: \"critical\"\n    description: \"Overall test status\"\n    default_value: \"unknown\"\n</code></pre>"},{"location":"guides/developer/#kpi-configuration-fields","title":"KPI Configuration Fields","text":"Field Description Required Values <code>name</code> Human-readable name Yes String <code>type</code> Data type Yes <code>numeric</code>, <code>string</code>, <code>boolean</code>, <code>list</code> <code>validation.operator</code> Comparison operator Yes <code>gte</code>, <code>lte</code>, <code>eq</code>, <code>ne</code>, <code>gt</code>, <code>lt</code> <code>validation.reference</code> Expected value/threshold Yes Varies by type <code>validation.enabled</code> Enable validation No <code>true</code> (default), <code>false</code> <code>unit</code> Measurement unit No String (e.g., \"ms\", \"fps\") <code>severity</code> Impact level No <code>critical</code>, <code>major</code>, <code>normal</code>, <code>minor</code> <code>description</code> KPI description No String <code>default_value</code> Fallback value No Varies by type"},{"location":"guides/developer/#kpi-operators","title":"KPI Operators","text":"Operator Description Example <code>gte</code> Greater than or equal <code>actual &gt;= reference</code> <code>lte</code> Less than or equal <code>actual &lt;= reference</code> <code>gt</code> Greater than <code>actual &gt; reference</code> <code>lt</code> Less than <code>actual &lt; reference</code> <code>eq</code> Equal to <code>actual == reference</code> <code>ne</code> Not equal to <code>actual != reference</code>"},{"location":"guides/developer/#referencing-kpis-in-tests","title":"Referencing KPIs in Tests","text":"<pre><code>tests:\n  test_inference:\n    params:\n      - test_id: \"INF-001\"\n        display_name: \"Inference Test\"\n        kpi_refs:\n          - inference_time\n          - accuracy\n          - status\n</code></pre>"},{"location":"guides/developer/#kpi-validation-in-tests","title":"KPI Validation in Tests","text":"<pre><code># Validate results against KPIs\nvalidation_results = validate_test_results(\n    results=results,\n    configs=configs,\n    get_kpi_config=get_kpi_config,\n    test_name=test_name\n)\n\n# Check validation status\nif validation_results[\"passed\"]:\n    logger.info(\"All KPIs passed\")\nelif validation_results[\"skipped\"]:\n    logger.info(f\"KPI validation skipped: {validation_results['skip_reason']}\")\nelse:\n    logger.error(\"Some KPIs failed\")\n    for kpi_name, kpi_result in validation_results[\"validations\"].items():\n        if not kpi_result[\"passed\"]:\n            logger.error(f\"  {kpi_name}: {kpi_result}\")\n</code></pre>"},{"location":"guides/developer/#validation-result-structure","title":"Validation Result Structure","text":"<pre><code>{\n    \"passed\": False,\n    \"skipped\": False,\n    \"skip_reason\": None,\n    \"validations\": {\n        \"inference_time\": {\n            \"passed\": True,\n            \"actual_value\": 0.45,\n            \"expected_value\": \"&lt;= 0.5\",\n            \"unit\": \"seconds\",\n            \"operator\": \"lte\",\n            \"severity\": \"major\"\n        },\n        \"accuracy\": {\n            \"passed\": False,\n            \"actual_value\": 0.85,\n            \"expected_value\": \"&gt;= 0.90\",\n            \"unit\": \"percentage\",\n            \"operator\": \"gte\",\n            \"severity\": \"critical\"\n        }\n    }\n}\n</code></pre>"},{"location":"guides/developer/#disabling-kpi-validation","title":"Disabling KPI Validation","text":"<pre><code>kpi:\n  optional_metric:\n    name: \"Optional Metric\"\n    type: \"numeric\"\n    validation:\n      operator: \"gte\"\n      reference: 100.0\n      enabled: false  # Metric collected but not validated\n</code></pre>"},{"location":"guides/developer/#asset-management","title":"Asset Management","text":"<p>The framework provides asset management for models, videos, and files.</p>"},{"location":"guides/developer/#asset-types","title":"Asset Types","text":""},{"location":"guides/developer/#video-assets","title":"Video Assets","text":"<pre><code>assets:\n  - id: \"video_sample\"\n    type: \"video\"\n    name: \"sample_1920_1080_30fps.h264\"\n    url: \"https://example.com/video.mp4\"\n    sha256: \"abc123...\"\n    width: 1920      # Resize to this width\n    height: 1080     # Resize to this height\n    fps: 30          # Convert to this framerate\n    codec: \"h264\"    # Convert to this codec (h264, h265)\n    duration: 30     # Trim to this duration (seconds)\n    loop: 120        # Loop video to achieve this duration\n</code></pre>"},{"location":"guides/developer/#model-assets","title":"Model Assets","text":"<pre><code>assets:\n  # Ultralytics model\n  - id: \"yolo11n\"\n    type: \"model\"\n    source: \"ultralytics\"\n    precision: \"int8\"\n    format: \"pt\"\n    export_args:\n      dynamic: true\n      half: true\n\n  # KaggleHub model\n  - id: \"resnet-50\"\n    type: \"model\"\n    source: \"kagglehub\"\n    precision: \"int8\"\n    format: \"openvino\"\n    kaggle_handle: \"google/resnet-v1/tensorFlow2/50-classification\"\n    convert_args:\n      input_shape: [1, 224, 224, 3]\n    quantize_args:\n      calibration_samples: 512\n</code></pre>"},{"location":"guides/developer/#file-assets","title":"File Assets","text":"<pre><code>assets:\n  - id: \"config_file\"\n    type: \"file\"\n    url: \"https://example.com/config.json\"\n    sha256: \"def456...\"\n    path: \"./configs/model.json\"\n</code></pre>"},{"location":"guides/developer/#using-assets-in-tests","title":"Using Assets in Tests","text":"<p>Assets are automatically prepared when specified in profile configurations. Access them in your test:</p> <pre><code># Assets prepared in standard locations\nmodels_dir = os.path.join(data_dir, \"models\")\nvideos_dir = os.path.join(data_dir, \"videos\")\n\n# Model path\nmodel_path = os.path.join(models_dir, \"yolo11n\", \"int8\", \"yolo11n.xml\")\n\n# Video path\nvideo_path = os.path.join(videos_dir, \"sample_1920_1080_30fps.h264\")\n</code></pre>"},{"location":"guides/developer/#best-practices","title":"Best Practices","text":""},{"location":"guides/developer/#test-design","title":"Test Design","text":"<ol> <li>Follow the 7-step pattern - Maintain consistency across all tests</li> <li>Use descriptive test IDs - Format: <code>{SUITE}-{NUM}</code> (e.g., <code>VSN-001</code>)</li> <li>Leverage caching - Tests should support both cached and non-cached execution</li> <li>Validate requirements early - Call <code>validate_system_requirements_from_configs</code> first</li> <li>Use proper logging - Log important steps at appropriate levels</li> <li>Handle cleanup - Always clean up resources (containers, files, processes)</li> </ol>"},{"location":"guides/developer/#configuration","title":"Configuration","text":"<ol> <li>Define clear KPIs - Set realistic thresholds based on baseline measurements</li> <li>Use requirement flags - Leverage existing flags instead of custom validation</li> <li>Document parameters - Add descriptions to test parameters</li> <li>Version profiles - Include version numbers in profile configurations</li> <li>Organize profiles - Use appropriate directories (qualifications/, suites/, verticals/)</li> </ol>"},{"location":"guides/developer/#error-handling","title":"Error Handling","text":"<ol> <li>Use pytest mechanisms - <code>pytest.skip()</code>, <code>pytest.fail()</code>, <code>pytest.xfail()</code></li> <li>Log errors clearly - Include context and suggested fixes</li> <li>Attach diagnostics - Use Allure attachments for logs and screenshots</li> <li>Clean up on failure - Use try/finally blocks for cleanup</li> </ol>"},{"location":"guides/developer/#performance","title":"Performance","text":"<ol> <li>Cache expensive operations - Model downloads, conversions, compilations</li> <li>Use timeouts - Set appropriate timeouts for long-running operations</li> <li>Parallelize when possible - Use thread pools for multi-device tests</li> <li>Monitor resources - Track memory and storage usage</li> </ol>"},{"location":"guides/developer/#security","title":"Security","text":"<ol> <li>Validate inputs - Use allow-lists for user-provided values</li> <li>Avoid shell=True - Use list-based subprocess calls</li> <li>Set file permissions - Use restrictive permissions (0o750, 0o770)</li> <li>Handle secrets safely - Never log sensitive information</li> </ol>"},{"location":"guides/developer/#advanced-topics","title":"Advanced Topics","text":""},{"location":"guides/developer/#custom-fixtures","title":"Custom Fixtures","text":"<p>Create test-specific fixtures in a <code>conftest.py</code> file:</p> <pre><code># src/esq/suites/my_domain/conftest.py\nimport pytest\n\n@pytest.fixture(scope=\"session\")\ndef shared_resource():\n    \"\"\"Fixture shared across all tests in this suite.\"\"\"\n    resource = setup_resource()\n    yield resource\n    cleanup_resource(resource)\n\n@pytest.fixture\ndef test_specific_resource(request):\n    \"\"\"Fixture for individual tests.\"\"\"\n    return create_resource()\n</code></pre>"},{"location":"guides/developer/#multi-device-testing","title":"Multi-Device Testing","text":"<p>Test across multiple devices efficiently:</p> <pre><code>from concurrent.futures import ThreadPoolExecutor, as_completed\nfrom sysagent.utils.system.ov_helper import get_available_devices_by_category\n\n# Get available devices\ndevice_dict = get_available_devices_by_category(\n    device_categories=[\"cpu\", \"igpu\", \"dgpu\"]\n)\n\n# Execute tests in parallel\nwith ThreadPoolExecutor(max_workers=len(device_dict)) as executor:\n    futures = {\n        executor.submit(run_test_on_device, device_id): device_id\n        for device_id in device_dict.keys()\n    }\n\n    for future in as_completed(futures):\n        device_id = futures[future]\n        try:\n            result = future.result()\n            results.metrics[f\"throughput_{device_id}\"] = Metrics(\n                value=result[\"throughput\"],\n                unit=\"fps\"\n            )\n        except Exception as e:\n            logger.error(f\"Test failed on {device_id}: {e}\")\n</code></pre>"},{"location":"guides/developer/#docker-integration","title":"Docker Integration","text":"<p>Use Docker for isolated test environments:</p> <pre><code>from sysagent.utils.infrastructure import DockerClient\n\ndocker_client = DockerClient()\n\n# Build image\nbuild_result = docker_client.build_image(\n    path=dockerfile_dir,\n    tag=\"my-test-image:latest\",\n    nocache=False\n)\n\n# Run container\ncontainer_result = docker_client.run_container(\n    image=\"my-test-image:latest\",\n    command=[\"python\", \"test_script.py\"],\n    volumes={\n        \"/host/path\": {\"bind\": \"/container/path\", \"mode\": \"rw\"}\n    },\n    environment={\"VAR\": \"value\"},\n    timeout=300\n)\n\n# Parse results\noutput = container_result.get(\"output\", \"\")\nexit_code = container_result.get(\"exit_code\", -1)\n</code></pre>"},{"location":"guides/developer/#profile-inheritance","title":"Profile Inheritance","text":"<p>Profiles can extend other profiles:</p> <pre><code># base_profile.yml\nname: \"profile.base\"\nparams:\n  requirements:\n    cpu_min_cores: 4\n    memory_min_gb: 8.0\n\n# extended_profile.yml\nextends: \"profile.base\"\nname: \"profile.extended\"\nparams:\n  requirements:\n    cpu_min_cores: 8  # Override\n    dgpu_required: true  # Add new requirement\n</code></pre>"},{"location":"guides/developer/#custom-metrics-and-aggregation","title":"Custom Metrics and Aggregation","text":"<p>Implement complex metric calculations:</p> <pre><code>def aggregate_device_metrics(device_results):\n    \"\"\"Aggregate metrics across multiple devices.\"\"\"\n    total_throughput = sum(r[\"throughput\"] for r in device_results.values())\n    avg_latency = sum(r[\"latency\"] for r in device_results.values()) / len(device_results)\n\n    return {\n        \"total_throughput\": total_throughput,\n        \"avg_latency\": avg_latency,\n        \"device_count\": len(device_results)\n    }\n\n# Use in test\nfor device_id, device_result in device_results.items():\n    results.metrics[f\"throughput_{device_id}\"] = Metrics(\n        value=device_result[\"throughput\"],\n        unit=\"fps\"\n    )\n\naggregated = aggregate_device_metrics(device_results)\nresults.metrics[\"total_throughput\"] = Metrics(\n    value=aggregated[\"total_throughput\"],\n    unit=\"fps\",\n    is_key_metric=True\n)\n</code></pre>"},{"location":"guides/developer/#iteration-data-and-visualizations","title":"Iteration Data and Visualizations","text":"<p>Track per-iteration metrics for detailed analysis:</p> <pre><code>iteration_data = {\n    \"iterations\": [],\n    \"throughput\": [],\n    \"latency\": []\n}\n\nfor i in range(num_iterations):\n    result = run_iteration()\n    iteration_data[\"iterations\"].append(i)\n    iteration_data[\"throughput\"].append(result[\"throughput\"])\n    iteration_data[\"latency\"].append(result[\"latency\"])\n\n# Summarize with visualizations\nsummarize_test_results(\n    results=results,\n    test_name=test_name,\n    iteration_data=iteration_data,\n    enable_visualizations=True,\n    configs=configs,\n    get_kpi_config=get_kpi_config\n)\n</code></pre>"},{"location":"guides/developer/#next-steps","title":"Next Steps","text":"<ol> <li>Review existing tests - Study tests in <code>src/esq/suites/ai/</code> for real-world examples</li> <li>Create your first test - Follow the quick start guide above</li> <li>Run tests - Use <code>esq -v run --profile your-profile</code> to execute</li> <li>Review results - Check JSON summary and Allure report</li> <li>Iterate - Refine your tests based on results and requirements</li> </ol> <p>For more information: - Developer Quick Reference - Cheat sheet for common tasks - Quick Start Guide - Troubleshooting Guide - API Reference</p> <p>Questions or Issues? - Open an issue on GitHub - Review existing tests for examples - Check the troubleshooting guide</p>"},{"location":"guides/optimization/","title":"Optimization Guide","text":"<p>This section provides comprehensive optimization guide.</p> <p>Documentation in progress.</p>"},{"location":"guides/troubleshooting/","title":"Troubleshooting Guide","text":"<p>This section provides comprehensive troubleshooting guide.</p> <p>Documentation in progress.</p>"}]}